# -*- coding: utf-8 -*-
"""Group28 HW2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16oG6Cy_4NErsM2sEEawnHKtG0VKduB5e

## Important packages
"""

# Commented out IPython magic to ensure Python compatibility.
#Import important Functions 
import pandas as pd
import numpy as np
import sklearn as sk
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

"""## **Functions**"""

#checking the accuracy of the model 
from sklearn.metrics import accuracy_score
def acc(Test , Predect):
  acc = accuracy_score(Test, Predect)
  return acc * 100

#ploting Decision Boundaries
import matplotlib.pyplot as plt

def decision_boundaries (trainx ,trainy, model, title):
  plt.figure(figsize = (10, 5))
  plt.scatter(trainx[trainy == 0,0], trainx[trainy == 0,1], c='y', marker='v', label=0)
  plt.scatter(trainx[trainy == 1,0], trainx[trainy == 1,1], c='r', marker='o', label=1)
  plt.scatter(trainx[trainy == 2,0], trainx[trainy == 2,1], c='b', marker='*', label=2)

  x_min, x_max = trainx[:, 0].min() - 0.1, trainx[:, 0].max() + 0.1
  y_min, y_max = trainx[:, 1].min() - 0.1, trainx[:, 1].max() + 0.1
  xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))

  z = model.predict(np.c_[xx.ravel(), yy.ravel()])
  Z= z.reshape(xx.shape)
  plt.contourf(xx, yy, Z, alpha=0.5)

  plt.title(title)
  plt.xlabel('color_intensity')
  plt.ylabel('proline')
  plt.legend()
  plt.show()

def training_time (k,f, xtrain, ytrain):
  import time
  start = time.time()
  ff = KNeighborsClassifier(n_neighbors= k)
  X_train_sample = xtrain.sample(frac= f  , random_state=0)
  y_train_sample = ytrain.sample(frac=f  , random_state=0)
  ff.fit(X_train_sample, y_train_sample)
  stop = time.time()
  time = stop - start
  print(f"Training time: {stop - start}s")
  return time

def prediction_time (k,f, xtrain, ytrain, xtest):
  import time
  
  ff = KNeighborsClassifier(n_neighbors= k)
  X_train_sample = xtrain.sample(frac= f  , random_state=42)
  y_train_sample = ytrain.sample(frac=f  , random_state=42)
  ff.fit(X_train_sample, y_train_sample)
  start = time.time()
  t = ff.predict(xtest)
  stop = time.time()
  time = stop - start
  print(f"Prediction time: {stop - start}s")
  return time

"""## **na¨ıve Bayesian classifier**"""

#Loading the data
from sklearn.datasets import load_wine
data = load_wine()

#convert the data into pandas DataFrame
wine_data= pd.DataFrame(data= np.c_[data['data'], data['target']],
                 columns= data['feature_names'] + ['target'])
wine_data['target'] = wine_data['target'].astype(int)
wine_data.head()

#spliting the data into training data and testing data
x=wine_data.drop('target',axis=1)
y=wine_data['target']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=2)

#applying naive_bayes classifier
from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
y_pred = gnb.fit(x_train, y_train).predict(x_test)


#getting classification report of the model
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

"""**Decision surface**"""

from sklearn.feature_selection import SelectKBest ,chi2
#using SelectKBest to choose two features
x_new = SelectKBest(chi2, k=2).fit_transform(x,y)
#results shown 'color_intensity' and 'proline'


from sklearn.model_selection import train_test_split
x_trainn,x_testn,y_trainn,y_testn = train_test_split(x_new,y,test_size=0.2,random_state=0) 

from sklearn.naive_bayes import GaussianNB
gnb_new = GaussianNB()
gnb_new.fit(x_trainn, y_trainn)

y_pred_new = gnb_new.predict(x_testn)

decision_boundaries (x_trainn ,y_trainn, gnb_new, 'decision boundaries')

"""## **KNN classifier**"""

#Load the car evaluation dataset
!gdown --id 1zb7NF4eRg0_S48GE3ZhcKDsXvXs7ZkmM

car_data= pd.read_csv('/content/car_evaluation.csv')
car_data = car_data.sample(frac=1  , random_state=2).reset_index(drop=True)
car_data.head()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()
data_columns = car_data.columns

for feature in data_columns :
  car_data[feature] = le.fit_transform(car_data[feature])

car_data.head()

#split the training data into xtrain and ytrain
xtrain = car_data.iloc[:, :-1]
ytrain = car_data.iloc[:,-1]


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(xtrain, ytrain ,test_size=428/1728, random_state = 2)
X_train, x_cv, y_train, y_cv = train_test_split(X_train,y_train,test_size =300/1300,random_state = 2)

print('Training set length: {} Row'.format(len(X_train)))
print('Validation set length: {} Row'.format(len(x_cv)))
print('Testing set length: {} Row'.format(len(X_test)))

from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors= 2)

accur  = []
for samples_percentage in np.arange(0.1, 1.1, 0.10) :
  X_train_sample = X_train.sample(frac= samples_percentage  , random_state=2)
  y_train_sample = y_train.sample(frac= samples_percentage  , random_state=2)
  neigh.fit(X_train_sample, y_train_sample)
  validation_predicted = neigh.predict(x_cv)
  testing_predicted = neigh.predict(X_test)
  
  acc_test = acc(y_test , testing_predicted)
  acc_validation = acc(y_cv , validation_predicted)
  accur.append({
      'samples percentage':('{}%'.format(int(samples_percentage *100))),
      'test accuracy' : acc_test , 
      'validation accuracy' : acc_validation 
      })
accuracy =  pd.DataFrame.from_dict(accur) 
print(accuracy)

fig = plt.figure(figsize = (10, 5))
portion = list(np.arange(0.1, 1.1, 0.10))
plt.plot(portion, list(accuracy['test accuracy']), label = "test accuracy", 
         color='green', linestyle='dashed', linewidth = 1, marker='o', markerfacecolor='blue', markersize=12)
plt.plot(portion, list(accuracy['validation accuracy']), label = "validation accuracy", 
         color='red', linestyle='dashed', linewidth = 1, marker='o', markerfacecolor='teal', markersize=12 )
# plt.title('title')
plt.xlabel('portion')
plt.ylabel('accuracy score')
plt.legend()
plt.show()

li  = []
for i in range(1, 11, 1) :
  KNN = KNeighborsClassifier(n_neighbors= i)
  KNN.fit(X_train, y_train)
  t = KNN.predict(X_test)
  v = KNN.predict(x_cv)

  ac_test = acc(t , y_test)
  ac_val = acc(v , y_cv)

  li.append({
      'n_neighbors': i ,
      'test accuracy' : ac_test , 
      'validation accuracy' : ac_val 
      })
K_varies =  pd.DataFrame.from_dict(li).reset_index(drop=True)
K_varies.sort_values(by = ['validation accuracy', 'test accuracy'] , ascending = False)

fig = plt.figure(figsize = (10, 5))
portion = list(np.arange(1, 11, 1))
plt.plot(portion, list(K_varies['validation accuracy']), label = "validation accuracy" ,
         color='red', linestyle='dashed', linewidth = 1, marker='o', markerfacecolor='teal', markersize=12 )
plt.title('accuracy curve on the validation')
plt.xlabel('K value')
plt.ylabel('accuracy score')
plt.legend()
plt.show()

fig = plt.figure(figsize = (10, 5))
portion = list(np.arange(1, 11, 1))
plt.plot(portion, list(K_varies['test accuracy']), label = "test accuracy" , 
         color='green', linestyle='dashed', linewidth = 1, marker='o', markerfacecolor='blue', markersize=12)
plt.title('accuracy curve on the test')
plt.xlabel('K value')
plt.ylabel('accuracy score')
plt.legend()
plt.show()

#case1 -> 10% of the whole training set and K = 2
case1 = training_time(2 , 0.1 , X_train , y_train)

#case2 -> 100% of the whole training set and K = 2
case2 = training_time(2 , 1 , X_train , y_train)

#case3 -> 10% of the whole training set and K = 10
case3 = training_time(10 , 0.1 , X_train , y_train)

#case4 -> 100% of the whole training set and K = 10
case4 = training_time(10 , 1 , X_train , y_train)

fig = plt.figure(figsize = (10, 5))
labels = ['case1', 'case2', 'case3', 'case4']
times = [case1,case2,case3,case4]
plt.bar(labels,times, color ='tab:red',width = 0.5)
plt.title('Training time ') 
plt.xlabel('cases')
plt.ylabel('Training time')
plt.show()

#case1 -> 10% of the whole prediction set and K = 2
case11 = prediction_time(2 , 0.1 , X_train , y_train, X_test)

#case2 -> 100% of the whole prediction set and K = 2
case22 = prediction_time(2 , 1 , X_train , y_train, X_test)

#case3 -> 10% of the whole prediction set and K = 10
case33 = prediction_time(10 , 0.1 , X_train , y_train, X_test)

#case4 -> 100% of the whole prediction set and K = 10
case44 = prediction_time(10 , 1 , X_train , y_train, X_test)

fig = plt.figure(figsize = (10, 5))
labels = ['case1', 'case2', 'case3', 'case4']
times = [case11,case22,case33,case44]
plt.bar(labels,times, color ='tab:red',width = 0.5)
plt.title('prediction time ')
plt.xlabel('cases')
plt.ylabel('prediction time')
plt.show()

